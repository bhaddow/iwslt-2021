# @package _global_

common:
  seed: 1
  user_dir: ${env:IWSLT_ROOT}/fairseq_modules/
  tensorboard_logdir: ${env:TB_ROOT}/iwslt21/

task:
  _name: speech_to_text_iwslt21
  data: ${env:DATA_ROOT}
  max_source_positions: 960_000   # 60s @ 16kHz
  max_target_positions: 1024
  normalize: True
  da_p_augm: 0.8
  da_tempo: 0.85,1.3
  da_pitch: -300,300
  da_echo_delay: 20,200
  da_echo_decay: 0.05,0.2
  sample_ratios: 1,0.3,1,0.3,1

dataset:
  train_subset: train_mustc,train_covost,train_europarlst,train_dev_covost,train_dev_europarlst
  valid_subset: dev_mustc
  num_workers: 6
  max_tokens: 960_000  # 3_200_000              # 200s @ 16kHz

criterion:
  _name: label_smoothed_cross_entropy
  report_accuracy: True
  ignore_prefix_size: 1

checkpoint:
  save_dir: ${env:SAVE_DIR}

optimization:
  lr: [2e-3]
  update_freq: [8]
  max_update: 100000
  clip_norm: 10.0

optimizer:
  _name: adam

lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 10000

model:
  _name: wav2vec_seq2seq_iwslt21
  w2v_path: ${env:WAV2VEC_ROOT}/wav2vec_vox_960h_pl.pt
  load_pretrained_decoder_from: ${env:MBART_ROOT}/model.pt
  autoregressive: True

  freeze_layers: encoder.feat_extr,encoder.ffn,decoder.embedding,decoder.self_attn,decoder.ffn
  adapter_dim: 4096
  len_adaptor_kernel_sizes: 3,3,3
  len_adaptor_channels: 1024

  decoder_embed_dim: 1024
  decoder_output_dim: 1024
  decoder_ffn_embed_dim: 4096
  decoder_layers: 12
  decoder_attention_heads: 16
  decoder_learned_pos: True
  decoder_normalize_before: True
  decoder_dropout: 0.3
  decoder_attention_dropout: 0.1
  decoder_activation_dropout: 0.0
  share_decoder_input_output_embed: True
  max_target_positions: "${task.max_target_positions}"
