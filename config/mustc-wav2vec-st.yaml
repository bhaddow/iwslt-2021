# @package _global_

common:
  seed: 1
  user_dir: ${env:IWSLT_ROOT}/fairseq_modules/
  tensorboard_logdir: ${env:TB_ROOT}/iwslt21/
  fp16: True

task:
  _name: speech_to_text_iwslt21
  data: ${env:DATA_ROOT}
  max_source_positions: 960_000   # 60s @ 16kHz
  max_target_positions: 1024
  normalize: True
  da_p_augm: 0.8
  da_tempo: 0.85,1.3
  da_pitch: -300,300
  da_echo_delay: 20,200
  da_echo_decay: 0.05,0.2
  sample_ratios: 1,0.3,1,0.3,1

distributed_training:
  find_unused_parameters: True

dataset:
  train_subset: train_mustc,train_covost,train_europarlst,train_dev_covost,train_dev_europarlst
  valid_subset: dev_mustc
  num_workers: 0
  max_tokens: 480_000  # 3_200_000              # 200s @ 16kHz
  max_tokens_valid: 960_000
  max_valid_steps: 4
  skip_invalid_size_inputs_valid_test: True

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.2
  ignore_prefix_size: 1

checkpoint:
  save_dir: ${env:SAVE_DIR}
  save_interval_updates: 20
  keep_interval_updates: 20

optimization:
    lr: [5e-4]
    max_update: 20
    update_freq: [8]
    sentence_avg: True
    clip_norm: 10.0


optimizer:
    _name: adam
    adam_betas: (0.9,0.98)
    adam_eps: 1e-08

lr_scheduler:
    _name: tri_stage
    phase_ratio: [0.1, 0.4, 0.5]
    final_lr_scale: 0.05

model:
  _name: wav2vec_seq2seq_iwslt21
  w2v_path: ${env:WAV2VEC_ROOT}/wav2vec_vox_960h_pl.pt
  load_pretrained_decoder_from: ${env:MBART_ROOT}/model.pt
  autoregressive: True

  freeze_layers: encoder.feat_extr,encoder.ffn,decoder.embedding,decoder.self_attn,decoder.ffn
  adapter_dim: 4096
  len_adaptor_kernel_sizes: 3,3,3
  len_adaptor_channels: 1024

  decoder_embed_dim: 1024
  decoder_output_dim: 1024
  decoder_ffn_embed_dim: 4096
  decoder_layers: 12
  decoder_attention_heads: 16
  decoder_learned_pos: True
  decoder_normalize_before: True
  decoder_dropout: 0.3
  decoder_attention_dropout: 0.1
  decoder_activation_dropout: 0.0
  share_decoder_input_output_embed: True
  max_target_positions: "${task.max_target_positions}"